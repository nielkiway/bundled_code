{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code used for creating, training and validating the CNN-Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs with tf 1.12-GPU\n",
    "# here all the required modules are imported \n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os \n",
    "import random\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, plot_confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "#from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is used to load the data into RAM\n",
    "\n",
    "# list of dirs containing the data - one for the porosity labeled data and one for the no_porosity_labeled data\n",
    "dirs = [ '/home/jan/Documents/Diplomarbeit/Trainingsdaten/datasets_new/100µm/arrays_sorted_for_parameters/3_layer/standard/porosity',\n",
    "         '/home/jan/Documents/Diplomarbeit/Trainingsdaten/datasets_new/100µm/arrays_sorted_for_parameters/3_layer/standard/no_porosity']\n",
    "\n",
    "\n",
    "length = 1877 # number of samples per category \n",
    "random_seed = 22 \n",
    "image_paths = []\n",
    "show_time = True  # can be toggled to show the time for loading the data into RAM \n",
    "\n",
    "if show_time:\n",
    "    start_time = time.time()\n",
    "\n",
    "# creating a list with paths of all files in each folder     \n",
    "for d in dirs: \n",
    "    cur_paths = []\n",
    "    for path in os.listdir(d):\n",
    "        full_path = os.path.join(d,path)  # adding the full path as the label is stored in the path\n",
    "        cur_paths.append(full_path)\n",
    "    \n",
    "    # randomly shuffling the created list and cutting it to the desired length\n",
    "    # added to the code for being able to deal with non pre-balanced data sets - if folders both have same length nothing happens at this point\n",
    "    random.seed(random_seed) \n",
    "    random.shuffle(cur_paths)\n",
    "    cur_paths = cur_paths[:length] \n",
    "    \n",
    "    image_paths.append(cur_paths) # appending the cut list to a new list containing all the final paths        \n",
    "\n",
    "image_paths = [val for sublist in image_paths for val in sublist] # getting the seperate image paths out of the list\n",
    "random.seed(random_seed)\n",
    "random.shuffle(image_paths) # randomly shuffling the list containing the final paths\n",
    "\n",
    "labels = []\n",
    "data = []\n",
    "# generating the label from the second last part of path for every element in the list and adding it to labels-list\n",
    "# loading the np array, resizing and storing it to data-list\n",
    "\n",
    "for path in image_paths:\n",
    "    label = path.split(os.path.sep)[-2] # here it is 'porosity' or 'no_porosity' \n",
    "    # additionally to the pure label the whole path is stored to make a traceback of model_behaviour possible \n",
    "    # the number needs to be changed according to the link to output something like '/porosity/ZP...'\n",
    "    labels.append([label,path[97:]])\n",
    "    array = np.load(path)\n",
    "    # img = Image.fromarray(array)   # could be added if resizing operation was necessary\n",
    "    # img = img.resize((128, 128), PIL.Image.LANCZOS)\n",
    "    # array_resized = np.array(img)\n",
    "    data.append(array)\n",
    "    \n",
    "# scale the raw pixel intensities to the range [0, 1] and making arrays out of lists for sklearn-functions\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# splitting data and labels in train, valid and test set in ratio (0.75, 0.125, 0.125)\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "(validX, testX, validY, testY) = train_test_split(testX, testY, test_size=0.5, random_state=42)\n",
    "\n",
    "# at this point the lables are an array with entries like ['porosity', '/porosity/ZP3_Slice00690_x:1_y:1.npy']\n",
    "# the following part splits up the array to generate labels and also a list with the full paths \n",
    "\n",
    "trainY_list = []\n",
    "trainY_path = []\n",
    "testY_list = []\n",
    "testY_path = []\n",
    "validY_list = []\n",
    "validY_path = []\n",
    "\n",
    "for row in trainY:\n",
    "    trainY_list.append(row[0])\n",
    "    trainY_path.append(row[1]) \n",
    "trainY = np.array(trainY_list)\n",
    "trainY_path = np.array(trainY_path)\n",
    "\n",
    "for row in testY:\n",
    "    testY_list.append(row[0])\n",
    "    testY_path.append(row[1]) \n",
    "testY = np.array(testY_list)\n",
    "testY_path = np.array(testY_path)\n",
    "\n",
    "for row in validY:\n",
    "    validY_list.append(row[0])\n",
    "    validY_path.append(row[1]) \n",
    "validY = np.array(validY_list)\n",
    "validY_path = np.array(validY_path)\n",
    "\n",
    "# the LabelBinarizer module is used to one hot encode the labels \n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "validY = lb.transform(validY)\n",
    "\n",
    "# now the Labels are arrays containing either a '0' for 'no_porosity' or a '1' for 'porosity'\n",
    "if show_time:\n",
    "    print(time.time()-start_time)\n",
    "    \n",
    "# the shapes of the different data sets are printed  \n",
    "print('shape training dataset: ' + str(trainX.shape))\n",
    "print('shape validation dataset: ' + str(validX.shape))\n",
    "print('shape test dataset: ' + str(testX.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case the 1 layer data is used it needs to be reshaped so it can be processed by the NN \n",
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1], trainX.shape[2], 1)\n",
    "validX = validX.reshape(validX.shape[0], validX.shape[1], validX.shape[2], 1)\n",
    "testX = testX.reshape(testX.shape[0], testX.shape[1], testX.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to keep the code for building the models clean \n",
    "\n",
    "def add_conv_module (n_filters, n_layers, regularization, kernel_initalization, activation, Batch_normalization = False, max_pool = False):\n",
    "    \n",
    "    # looping through the defined number of layers and adding Convolutional layers\n",
    "    # in all the models used in the work the kernel size was (3,3) and same-padding was used \n",
    "    for i in range (n_layers):\n",
    "        model.add(Conv2D(filters=n_filters,kernel_size=(3,3),padding=\"same\", activation=activation,\n",
    "                         kernel_initializer = kernel_initalization, kernel_regularizer = regularization))\n",
    "        if Batch_normalization == True:\n",
    "            model.add(BatchNormalization())\n",
    "    \n",
    "    # if desired a max pooling layer can be added to the end of block \n",
    "    if max_pool == True:\n",
    "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))      \n",
    "        if Batch_normalization == True:\n",
    "            model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_building\n",
    "# This part needs to be adjusted depending on the desired architecture of the model\n",
    "\n",
    "\n",
    "model_name = \"3layer_100µm_area_2x16_2x32_3x64\"\n",
    "\n",
    "# defining parameters\n",
    "regul = regularizers.l2(0.001)\n",
    "kernel_inil = \"he_normal\"\n",
    "model = Sequential()       \n",
    "inputShape = (220, 220, 3)\n",
    "activation = \"relu\"\n",
    "dropout_rate = 0.2 #4 #3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# This is the feature extractor part of the model \n",
    "# first conv_layer (special because of Input-Shape)\n",
    "model.add(Conv2D(input_shape=inputShape, filters=16,kernel_size=(3,3),padding=\"same\", activation=activation,\n",
    "                         kernel_initializer = kernel_inil, kernel_regularizer = regul))\n",
    "model.add(BatchNormalization())\n",
    "# further conv_layers\n",
    "add_conv_module(n_filters = 16, n_layers= 1, regularization = regul, kernel_initalization = kernel_inil,\n",
    "                activation = activation, Batch_normalization = True, max_pool = True)\n",
    "\n",
    "add_conv_module(n_filters = 32, n_layers= 2, regularization = regul, kernel_initalization = kernel_inil,\n",
    "                activation = activation, Batch_normalization = True, max_pool = True)\n",
    "\n",
    "add_conv_module(n_filters = 64, n_layers= 3, regularization = regul, kernel_initalization = kernel_inil,\n",
    "                activation = activation, Batch_normalization = True, max_pool = True)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the classifier part of the model \n",
    "model.add(Dense(units=16,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(units=16,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(units = 1,activation=\"sigmoid\")) \n",
    "\n",
    "# building the model and showing the summary\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the number of epochs, the Batchsize, the optimizer and the Data Augmentation steps are defined \n",
    "# Finally the model is compiled \n",
    "\n",
    "EPOCHS = 500\n",
    "BS = 32\n",
    "opt = SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov = True)     #'Adam' # should be the default optimizer with standard setting according to Hands-On-ML\n",
    "aug = ImageDataGenerator(vertical_flip= True,horizontal_flip=True) # data is randomly flipped horizontally as well as vertically to prevent the NN from learning local dependecies aug = ImageDataGenerator(vertical_flip= True,horizontal_flip=True) # data is randomly flipped horizontally as well as vertically to prevent the NN from learning local dependecies \n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the actual training process is started \n",
    "\n",
    "# first a file is defined where the model_weights are store\n",
    "filepath=\"model_weights/{}_weights.best.hdf5\".format(model_name)\n",
    "\n",
    "# the checkpoint saved the model weights whenever the validation loss decreases \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# EarlyStopping is set up to stop training when validation loss isn't decreasing for a number of epochs defindes by the patience parameter\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# checkpoint and Early stopping are added to the callbacks list \n",
    "callbacks_list = [checkpoint,es]\n",
    "\n",
    "# Thbe model is fit to the training data and validated with the validation data \n",
    "J = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),validation_data=(validX, validY), steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is used for plotting the learning curve\n",
    "\n",
    "N = np.arange(0,44) # the number needs to be adjusted depending on the \n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure\n",
    "plt.plot(N, J.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, J.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, J.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, J.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy ({})\".format(model_name))\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is used for creating the confusion matrix and the classification report with the calculated metrics\n",
    "\n",
    "# As a first step the models best weights are loaded before making the predicitions on the so far held back test set\n",
    "model.load_weights(\"model_weights/{}_weights.best.hdf5\".format(model_name))\n",
    "\n",
    "# here the model predicts the class based on the data\n",
    "model_predictions = model.predict(testX)\n",
    "model_predictions =(model_predictions>0.5)\n",
    "\n",
    "cm = confusion_matrix(testY, model_predictions)\n",
    "cr = classification_report(testY, model_predictions)\n",
    "\n",
    "\n",
    "print(model_name)\n",
    "print(cm)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_models_for_work/100µm/Standardparameters/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
