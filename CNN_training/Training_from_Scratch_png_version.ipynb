{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code used for creating, training and validating the CNN-Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs with tf 1.12-GPU\n",
    "# here all the required modules are imported \n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "import random\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, plot_confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "#from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.87855529785156\n",
      "shape training dataset: (7218, 128, 128, 3)\n",
      "shape validation dataset: (1203, 128, 128, 3)\n",
      "shape test dataset: (1203, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# This block is used to load the data into RAM\n",
    "\n",
    "# list of dirs containing the data - one for the porosity labeled data and one for the no_porosity_labeled data\n",
    "dirs = [ '/home/jan/Documents/Diplomarbeit/Code_zusammengefasst/QM-Meltpool-Datenaufbereitung/RGB_area_images_all_slices_sorted/standard_orig/porosity',\n",
    "         '/home/jan/Documents/Diplomarbeit/Code_zusammengefasst/QM-Meltpool-Datenaufbereitung/RGB_area_images_all_slices_sorted/standard_orig/no_porosity']\n",
    "\n",
    "length = 4632 # number of samples per category \n",
    "random_seed = 22 \n",
    "image_paths = []\n",
    "show_time = True  # can be toggled to show the time for loading the data into RAM \n",
    "\n",
    "if show_time:\n",
    "    start_time = time.time()\n",
    "\n",
    "# creating a list with paths of all files in each folder     \n",
    "for d in dirs: \n",
    "    cur_paths = []\n",
    "    for path in os.listdir(d):\n",
    "        full_path = os.path.join(d,path)  # adding the full path as the label is stored in the path\n",
    "        cur_paths.append(full_path)\n",
    "    \n",
    "    # randomly shuffling the created list and cutting it to the desired length\n",
    "    # added to the code for being able to deal with non pre-balanced data sets - if folders both have same length nothing happens at this point\n",
    "    random.seed(random_seed) \n",
    "    random.shuffle(cur_paths)\n",
    "    #cur_paths = cur_paths[:length] \n",
    "    \n",
    "    image_paths.append(cur_paths) # appending the cut list to a new list containing all the final paths        \n",
    "\n",
    "image_paths = [val for sublist in image_paths for val in sublist] # getting the seperate image paths out of the list\n",
    "random.seed(random_seed)\n",
    "random.shuffle(image_paths) # randomly shuffling the list containing the final paths\n",
    "\n",
    "labels = []\n",
    "data = []\n",
    "# generating the label from the second last part of path for every element in the list and adding it to labels-list\n",
    "# loading the np array, resizing and storing it to data-list\n",
    "\n",
    "for path in image_paths:\n",
    "    label = path.split(os.path.sep)[-2] # here it is 'porosity' or 'no_porosity' \n",
    "    # additionally to the pure label the whole path is stored to make a traceback of model_behaviour possible \n",
    "    # the number needs to be changed according to the link to output something like '/porosity/ZP...'\n",
    "    labels.append([label,path[131:]])\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((128, 128), PIL.Image.LANCZOS) \n",
    "    raw_array = np.asarray(img)\n",
    "    array = cv2.cvtColor(raw_array, cv2.COLOR_BGRA2BGR)\n",
    "    data.append(array)\n",
    "    \n",
    "# scale the raw pixel intensities to the range [0, 1] and making arrays out of lists for sklearn-functions\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# splitting data and labels in train, valid and test set in ratio (0.75, 0.125, 0.125)\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "(validX, testX, validY, testY) = train_test_split(testX, testY, test_size=0.5, random_state=42)\n",
    "\n",
    "# at this point the lables are an array with entries like ['porosity', '/porosity/ZP3_Slice00690_x:1_y:1.npy']\n",
    "# the following part splits up the array to generate labels and also a list with the full paths \n",
    "\n",
    "trainY_list = []\n",
    "trainY_path = []\n",
    "testY_list = []\n",
    "testY_path = []\n",
    "validY_list = []\n",
    "validY_path = []\n",
    "\n",
    "for row in trainY:\n",
    "    trainY_list.append(row[0])\n",
    "    trainY_path.append(row[1]) \n",
    "trainY = np.array(trainY_list)\n",
    "trainY_path = np.array(trainY_path)\n",
    "\n",
    "for row in testY:\n",
    "    testY_list.append(row[0])\n",
    "    testY_path.append(row[1]) \n",
    "testY = np.array(testY_list)\n",
    "testY_path = np.array(testY_path)\n",
    "\n",
    "for row in validY:\n",
    "    validY_list.append(row[0])\n",
    "    validY_path.append(row[1]) \n",
    "validY = np.array(validY_list)\n",
    "validY_path = np.array(validY_path)\n",
    "\n",
    "# the LabelBinarizer module is used to one hot encode the labels \n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "validY = lb.transform(validY)\n",
    "\n",
    "# now the Labels are arrays containing either a '0' for 'no_porosity' or a '1' for 'porosity'\n",
    "if show_time:\n",
    "    print(time.time()-start_time)\n",
    "    \n",
    "# the shapes of the different data sets are printed  \n",
    "print('shape training dataset: ' + str(trainX.shape))\n",
    "print('shape validation dataset: ' + str(validX.shape))\n",
    "print('shape test dataset: ' + str(testX.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case the 1 layer data is used it needs to be reshaped so it can be processed by the NN \n",
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1], trainX.shape[2], 1)\n",
    "validX = validX.reshape(validX.shape[0], validX.shape[1], validX.shape[2], 1)\n",
    "testX = testX.reshape(testX.shape[0], testX.shape[1], testX.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to keep the code for building the models clean \n",
    "\n",
    "def add_conv_module (n_filters, n_layers, regularization, kernel_initalization, activation, Batch_normalization = False, max_pool = False):\n",
    "    \n",
    "    # looping through the defined number of layers and adding Convolutional layers\n",
    "    # in all the models used in the work the kernel size was (3,3) and same-padding was used \n",
    "    for i in range (n_layers):\n",
    "        model.add(Conv2D(filters=n_filters,kernel_size=(3,3),padding=\"same\", activation=activation,\n",
    "                         kernel_initializer = kernel_initalization, kernel_regularizer = regularization))\n",
    "        if Batch_normalization == True:\n",
    "            model.add(BatchNormalization())\n",
    "    \n",
    "    # if desired a max pooling layer can be added to the end of block \n",
    "    if max_pool == True:\n",
    "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))      \n",
    "        if Batch_normalization == True:\n",
    "            model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 16)      448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 16)      2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                262160    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 373,185\n",
      "Trainable params: 372,321\n",
      "Non-trainable params: 864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model_building\n",
    "# This part needs to be adjusted depending on the desired architecture of the model\n",
    "\n",
    "\n",
    "model_name = \"3layer_100µm_mixed_2x16_2x32_3x64\"\n",
    "\n",
    "# defining parameters\n",
    "regul = regularizers.l2(0.001)\n",
    "kernel_inil = \"he_normal\"\n",
    "model = Sequential()       \n",
    "inputShape = (128, 128, 3)\n",
    "activation = \"relu\"\n",
    "dropout_rate = 0.2 #4 #3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# This is the feature extractor part of the model \n",
    "# first conv_layer (special because of Input-Shape)\n",
    "model.add(Conv2D(input_shape=inputShape, filters=16,kernel_size=(3,3),padding=\"same\", activation=activation,\n",
    "                         kernel_initializer = kernel_inil, kernel_regularizer = regul))\n",
    "model.add(BatchNormalization())\n",
    "# further conv_layers\n",
    "add_conv_module(n_filters = 16, n_layers= 1, regularization = regul, kernel_initalization = kernel_inil,\n",
    "                activation = activation, Batch_normalization = True, max_pool = True)\n",
    "\n",
    "add_conv_module(n_filters = 32, n_layers= 2, regularization = regul, kernel_initalization = kernel_inil,\n",
    "                activation = activation, Batch_normalization = True, max_pool = True)\n",
    "\n",
    "add_conv_module(n_filters = 64, n_layers= 3, regularization = regul, kernel_initalization = kernel_inil,\n",
    "                activation = activation, Batch_normalization = True, max_pool = True)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the classifier part of the model \n",
    "model.add(Dense(units=16,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(units=16,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(units = 1,activation=\"sigmoid\")) \n",
    "\n",
    "# building the model and showing the summary\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the number of epochs, the Batchsize, the optimizer and the Data Augmentation steps are defined \n",
    "# Finally the model is compiled \n",
    "\n",
    "EPOCHS = 500\n",
    "BS = 32\n",
    "opt = SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov = True)     #'Adam' # should be the default optimizer with standard setting according to Hands-On-ML\n",
    "aug = ImageDataGenerator(vertical_flip= True,horizontal_flip=True) # data is randomly flipped horizontally as well as vertically to prevent the NN from learning local dependecies aug = ImageDataGenerator(vertical_flip= True,horizontal_flip=True) # data is randomly flipped horizontally as well as vertically to prevent the NN from learning local dependecies \n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "225/225 [==============================] - 289s 1s/step - loss: 1.2290 - accuracy: 0.6124 - val_loss: 1.3142 - val_accuracy: 0.5353\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.31417, saving model to model_weights/3layer_100µm_mixed_2x16_2x32_3x64_weights.best.hdf5\n",
      "Epoch 2/500\n",
      "225/225 [==============================] - 317s 1s/step - loss: 1.1373 - accuracy: 0.6723 - val_loss: 1.1889 - val_accuracy: 0.6035\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.31417 to 1.18889, saving model to model_weights/3layer_100µm_mixed_2x16_2x32_3x64_weights.best.hdf5\n",
      "Epoch 3/500\n",
      "225/225 [==============================] - 354s 2s/step - loss: 1.0759 - accuracy: 0.6877 - val_loss: 1.0480 - val_accuracy: 0.6883\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.18889 to 1.04797, saving model to model_weights/3layer_100µm_mixed_2x16_2x32_3x64_weights.best.hdf5\n",
      "Epoch 4/500\n",
      "225/225 [==============================] - 318s 1s/step - loss: 1.0327 - accuracy: 0.6927 - val_loss: 1.0157 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.04797 to 1.01569, saving model to model_weights/3layer_100µm_mixed_2x16_2x32_3x64_weights.best.hdf5\n",
      "Epoch 5/500\n",
      "225/225 [==============================] - 295s 1s/step - loss: 0.9881 - accuracy: 0.7041 - val_loss: 0.9735 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01569 to 0.97347, saving model to model_weights/3layer_100µm_mixed_2x16_2x32_3x64_weights.best.hdf5\n",
      "Epoch 6/500\n",
      "225/225 [==============================] - 288s 1s/step - loss: 0.9480 - accuracy: 0.7062 - val_loss: 1.1167 - val_accuracy: 0.5511\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.97347\n",
      "Epoch 7/500\n",
      "225/225 [==============================] - 275s 1s/step - loss: 0.9133 - accuracy: 0.7076 - val_loss: 1.2051 - val_accuracy: 0.5578\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.97347\n",
      "Epoch 8/500\n",
      "225/225 [==============================] - 270s 1s/step - loss: 0.8837 - accuracy: 0.7060 - val_loss: 0.9000 - val_accuracy: 0.6941\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.97347 to 0.90005, saving model to model_weights/3layer_100µm_mixed_2x16_2x32_3x64_weights.best.hdf5\n",
      "Epoch 9/500\n",
      "225/225 [==============================] - 276s 1s/step - loss: 0.8571 - accuracy: 0.7137 - val_loss: 1.1872 - val_accuracy: 0.5362\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.90005\n",
      "Epoch 10/500\n",
      "225/225 [==============================] - 297s 1s/step - loss: 0.8372 - accuracy: 0.7135 - val_loss: 0.8528 - val_accuracy: 0.6858\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.90005 to 0.85284, saving model to model_weights/3layer_100µm_mixed_2x16_2x32_3x64_weights.best.hdf5\n",
      "Epoch 11/500\n",
      "225/225 [==============================] - 308s 1s/step - loss: 0.8095 - accuracy: 0.7242 - val_loss: 1.5056 - val_accuracy: 0.5353\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.85284\n",
      "Epoch 12/500\n",
      "225/225 [==============================] - 284s 1s/step - loss: 0.7896 - accuracy: 0.7243 - val_loss: 0.8699 - val_accuracy: 0.6467\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.85284\n",
      "Epoch 13/500\n",
      "225/225 [==============================] - 296s 1s/step - loss: 0.7651 - accuracy: 0.7311 - val_loss: 0.8902 - val_accuracy: 0.6209\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.85284\n",
      "Epoch 14/500\n",
      "  2/225 [..............................] - ETA: 4:33 - loss: 0.7561 - accuracy: 0.7188"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8f4730095fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Thbe model is fit to the training data and validated with the validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \"\"\"\n\u001b[0;32m-> 1718\u001b[0;31m         return training_generator.fit_generator(\n\u001b[0m\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 outs = model.train_on_batch(x, y,\n\u001b[0m\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here the actual training process is started \n",
    "\n",
    "# first a file is defined where the model_weights are store\n",
    "filepath=\"model_weights/{}_weights.best.hdf5\".format(model_name)\n",
    "\n",
    "# the checkpoint saved the model weights whenever the validation loss decreases \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# EarlyStopping is set up to stop training when validation loss isn't decreasing for a number of epochs defindes by the patience parameter\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# checkpoint and Early stopping are added to the callbacks list \n",
    "callbacks_list = [checkpoint,es]\n",
    "\n",
    "# Thbe model is fit to the training data and validated with the validation data \n",
    "J = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),validation_data=(validX, validY), steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is used for plotting the learning curve\n",
    "\n",
    "N = np.arange(0,44) # the number needs to be adjusted depending on the \n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure\n",
    "plt.plot(N, J.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, J.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, J.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, J.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy ({})\".format(model_name))\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3layer_100µm_mixed_2x16_2x32_3x64\n",
      "[[278 228]\n",
      " [156 541]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.55      0.59       506\n",
      "           1       0.70      0.78      0.74       697\n",
      "\n",
      "    accuracy                           0.68      1203\n",
      "   macro avg       0.67      0.66      0.66      1203\n",
      "weighted avg       0.68      0.68      0.68      1203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This block is used for creating the confusion matrix and the classification report with the calculated metrics\n",
    "\n",
    "# As a first step the models best weights are loaded before making the predicitions on the so far held back test set\n",
    "model.load_weights(\"model_weights/{}_weights.best.hdf5\".format(model_name))\n",
    "\n",
    "# here the model predicts the class based on the data\n",
    "model_predictions = model.predict(testX)\n",
    "model_predictions =(model_predictions>0.5)\n",
    "\n",
    "cm = confusion_matrix(testY, model_predictions)\n",
    "cr = classification_report(testY, model_predictions)\n",
    "\n",
    "\n",
    "print(model_name)\n",
    "print(cm)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_models_for_work/100µm/Standardparameters/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
